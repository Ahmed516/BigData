Hi, interviewer, good evenning, firstly allow me to briefly talk about myself.
i'm guojia. and you can call me by my english name ahmed.  Since i graduated from SYSU with majar software engineering, i have been working as a big data developer and i have about 4 years experiences. In my recent job, I am mainly responsible for the design and development of the data line architecture of CCB's life, realtime-data & batchtime-data collection of hadoop clusters, data warehouse construction and so on. After the joint efforts of me and my teammates, we successfully constructed the CCBLIFE data platform ecology from 0 to 1. At the same time, i get a lot of benefits from it.
As a big data developer, i am good at data warehouse devepment, and i am familiar with mainstream technology in Bigdata, such as Hadoop, hive, Hbase, Kafka, flink, flume, filebeat, etc.
last but not least, i am a postive person and i have big passion for learning great technologies. i believe that i can bring very huge energe to my job and my life.
that's all, thank you!

【spark & Hadoop】
Spark is generally faster than Hadoop’s MapReduce for certain types of workloads. This is because Spark is designed to keep data in memory rather than writing it to disk, which can significantly reduce the amount of time it takes to process data. Spark can also cache intermediate data in memory, which can speed up iterative algorithms and interactive data analysis.

In contrast, Hadoop’s MapReduce writes intermediate data to disk between each stage of processing. This can add significant overhead for workloads that require multiple stages of processing. However, for very large datasets that do not fit in memory, Hadoop’s MapReduce can still be a good choice because it can scale out to a large number of machines and process data in parallel.

In summary, Spark can be faster than Hadoop’s MapReduce for certain types of workloads, particularly those that benefit from caching data in memory. However, the performance of both frameworks depends on many factors, including the size and complexity of the data being processed and the resources available in the cluster.

【spark RDD】
In summary, RDDs are a fundamental data structure in Spark that represents an immutable distributed collection of objects. They are partitioned across the nodes in a cluster and can be processed in parallel. RDDs support two types of operations: transformations and actions.

【YARN】
Apache Yarn is a resource manager that works with Hadoop and other applications to run them across a distributed architecture. 
The architecture of Apache Yarn consists of the following main components12:
1. Resource Manager: Runs on a master daemon and manages the resource allocation in the cluster. It has two sub-components: the Scheduler and the Applications Manager. The Scheduler is responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc. The Applications Manager is responsible for accepting job-submissions, negotiating the first container for executing the application specific ApplicationMaster and providing the service for restarting the ApplicationMaster container on failure.
2. Node Manager: They run on the slave daemons and are responsible for the execution of a task on every single Data Node. They also monitor the resource usage (cpu, memory, disk, network) of each container and report it to the Resource Manager.
3. Application Master: They are framework specific libraries that run on a container and are tasked with negotiating resources from the Resource Manager and working with the Node Manager(s) to execute and monitor the tasks. They also coordinate with the Resource Manager to deal with failures and completion of their applications.
4. Container: They are the basic unit of resource allocation in Yarn. They incorporate elements such as memory, cpu, disk, network etc. Each application runs inside one or more containers across different nodes.

【Java abstract class & interface】
Some key differences between an abstract class and an interface include:

An abstract class can have instance variables and constructors, while an interface cannot.
An abstract class can contain both abstract and concrete methods, while an interface can only contain abstract methods (and default methods in Java 8 and above).
A class can implement multiple interfaces, but can only extend one abstract class.
